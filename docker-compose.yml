version: "3.9"

services:
  # ----------------------------------------
  # 1. ClickHouse Data Warehouse
  # ----------------------------------------
  clickhouse-server:
    image: clickhouse/clickhouse-server
    container_name: clickhouse-server
    environment:
      # Credentials used by both Airflow and dbt
      CLICKHOUSE_USER: dbt_user
      CLICKHOUSE_PASSWORD: dbt_password
      CLICKHOUSE_DB: data_warehouse
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native client interface
    volumes:
      - ./clickhouse_data:/var/lib/clickhouse/ # Persist data
      - ./sql:/sql # Mount SQL scripts
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/ping"]
      interval: 10s       # Check less frequently
      timeout: 5s         # Timeout for each check
      retries: 30         # Allow more failed checks before giving up
      start_period: 30s   # <-- CRITICAL: Wait 30 seconds before even starting checks
    restart: always


  # ----------------------------------------
  # 2. dbt (Data Build Tool)
  # ----------------------------------------
  dbt:
    # Build dbt container from the custom dbt/Dockerfile
    build:
      context: .
      dockerfile: dbt/Dockerfile # Assumes dbt/Dockerfile exists
    image: my_project_dbt:latest # Custom image name
    container_name: dbt
    depends_on:
      clickhouse-server:
        condition: service_healthy # Wait for ClickHouse
    volumes:
      # Mount the entire project so dbt can see dbt_project.yml, models, etc.
      - .:/usr/app 
    working_dir: /usr/app # Set working directory to the project root
    environment:
      DBT_PROFILES_DIR: /usr/app/.dbt 
      # Environment variables for dbt profiles.yml connection
      CLICKHOUSE_HOST: clickhouse-server
      CLICKHOUSE_USER: dbt_user
      CLICKHOUSE_PASSWORD: dbt_password
      CLICKHOUSE_PORT: 9000
      CLICKHOUSE_DATABASE: data_warehouse
    tty: true
    command: tail -f /dev/null # Keep container running


  # ----------------------------------------
  # 3. Airflow Components (Custom Build)
  # ----------------------------------------
  airflow-db:
    container_name: airflow-db
    image: postgres:16.4
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./pgdata_airflow:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5
    restart: always

  # --- Custom Airflow Image Definition ---
  airflow-base:
    # Build Airflow containers from the custom Dockerfile in the root
    build:
      context: .
      dockerfile: Dockerfile # Assumes Dockerfile exists in root
    image: my_project_airflow:2.8.1 # Custom image name
    # The image is only built, not run as a service itself

  airflow-init:
    image: my_project_airflow:2.8.1 # Use the custom built image
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        /bin/bash -c "while !</dev/tcp/airflow-db/5432; do sleep 1; done;" 
        airflow db init
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags 
    depends_on:
      - airflow-db
    restart: "no"

  airflow-webserver:
    image: my_project_airflow:2.8.1 # Use the custom built image
    container_name: airflow-webserver
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      # Connection to ClickHouse for Airflow tasks/Hooks
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: clickhouse://dbt_user:dbt_password@clickhouse-server:9000/data_warehouse
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - .:/opt/airflow/project_root # Mount project root for dbt-run scripts etc.
    depends_on:
      - airflow-init
      - clickhouse-server
    restart: always

  airflow-scheduler:
    image: my_project_airflow:2.8.1 # Use the custom built image
    container_name: airflow-scheduler
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: clickhouse://dbt_user:dbt_password@clickhouse-server:9000/data_warehouse
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - .:/opt/airflow/project_root
    depends_on:
      - airflow-init
      - clickhouse-server
    restart: always

volumes:
  pgdata_airflow:
  clickhouse_data: